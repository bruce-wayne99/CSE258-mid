{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "import dateutil\n",
    "import functools\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(f):\n",
    "    for l in gzip.open(f):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from below:\n",
    "# https://cseweb.ucsd.edu/classes/fa21/cse258-b/files/\n",
    "dataset = list(parse(\"trainRecipes.json.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[:150000]\n",
    "valid = dataset[150000:175000]\n",
    "test = dataset[175000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'double delicious cookie bars',\n",
       " 'minutes': 40,\n",
       " 'contributor_id': '26865936',\n",
       " 'submitted': '2007-08-27',\n",
       " 'steps': 'preheat oven to 350f\\tin 13x9-inch baking pan , melt butter in oven\\tsprinkle crumbs evenly over butter\\tpour milk evenly over crumbs\\ttop with remaining ingredients\\tpress down firmly\\tbake 25-30 minutes or until lightly browned\\tcool completely , chill if desired , and cut into bars',\n",
       " 'description': 'from \"all time favorite recipes\". for fun, try substituting butterscotch or white chocolate chips for the semi-sweet and/or peanut butter chips. make sure you cool it completely or the bottom will crumble!',\n",
       " 'ingredients': ['butter',\n",
       "  'graham cracker crumbs',\n",
       "  'sweetened condensed milk',\n",
       "  'semi-sweet chocolate chips',\n",
       "  'peanut butter chips'],\n",
       " 'recipe_id': '98015212'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat1a(d):\n",
    "    return [len(d['steps']), len(d['ingredients'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month(d):\n",
    "    t = dateutil.parser.parse(d['submitted'])\n",
    "    return t.month\n",
    "\n",
    "def get_year(d):\n",
    "    t = dateutil.parser.parse(d['submitted'])\n",
    "    return t.year\n",
    "\n",
    "min_year = -1\n",
    "max_year = -1\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    year = get_year(dataset[i])\n",
    "    if min_year == -1:\n",
    "        min_year = year\n",
    "    else:\n",
    "        min_year = min(min_year, year)\n",
    "    if max_year == -1:\n",
    "        max_year = year\n",
    "    else:\n",
    "        max_year = max(max_year, year)\n",
    "\n",
    "num_years = max_year - min_year + 1\n",
    "num_months = 12\n",
    "\n",
    "num_year_encoding = num_years - 1\n",
    "num_months_encoding = num_months - 1\n",
    "\n",
    "def feat1b(d):\n",
    "    year = get_year(d)\n",
    "    month = get_month(d)\n",
    "    \n",
    "    year_encoding = [0 for i in range(num_year_encoding)]\n",
    "    month_encoding = [0 for i in range(num_months_encoding)]\n",
    "    \n",
    "    if year > min_year:\n",
    "        year_encoding[year-min_year-1] = 1\n",
    "    \n",
    "    if month > 1:\n",
    "        month_encoding[month-2] = 1\n",
    "    \n",
    "    feature = year_encoding + month_encoding\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredients_dict = defaultdict(int)\n",
    "for data in dataset:\n",
    "    ingredients = data['ingredients']\n",
    "    for ingredient in ingredients:\n",
    "        ingredients_dict[ingredient] += 1\n",
    "\n",
    "ingredients_list = []\n",
    "for ingredient in ingredients_dict:\n",
    "    ingredients_list.append((ingredients_dict[ingredient], ingredient))\n",
    "\n",
    "def ingredient_comparator(tuple1, tuple2):\n",
    "    if tuple1[0] > tuple2[0]:\n",
    "        return -1\n",
    "    elif tuple1[0] < tuple2[0]:\n",
    "        return 1\n",
    "    elif tuple1[1] < tuple2[1]:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "ingredients_list.sort(key=functools.cmp_to_key(ingredient_comparator))\n",
    "ingredients_idx_dict = {}\n",
    "for i in range(len(ingredients_list)):\n",
    "    ingredients_idx_dict[ingredients_list[i][1]] = i\n",
    "\n",
    "def feat1c(d):\n",
    "    binary_vector = [0 for _ in range(50)]\n",
    "    ingredients = d['ingredients']\n",
    "    for ingredient in ingredients:\n",
    "        if ingredients_idx_dict[ingredient] <= 49:\n",
    "            binary_vector[ingredients_idx_dict[ingredient]] = 1\n",
    "    return binary_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat(d, a = True, b = True, c = True):\n",
    "    # Hint: for Questions 1 and 2, might be useful to set up a function like this\n",
    "    #       which allows you to \"select\" which features are included\n",
    "    feature = [1]\n",
    "    if a:\n",
    "        feature += feat1a(d)\n",
    "    if b:\n",
    "        feature += feat1b(d)\n",
    "    if c:\n",
    "        feature += feat1c(d)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, ypred):\n",
    "    # Can use library if you prefer\n",
    "    diff = y - ypred\n",
    "    mse = (diff.T*diff / diff.shape[0]).tolist()[0][0]\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(a = True, b = True, c = True):\n",
    "    # Hint: might be useful to write this function which extracts features and \n",
    "    #       computes the performance of a particular model on those features\n",
    "    X_train = [feat(d, a, b, c) for d in train]\n",
    "    y_train = [d['minutes'] for d in train]\n",
    "    X_test = [feat(d, a, b, c) for d in test]\n",
    "    y_test = [d['minutes'] for d in test]\n",
    "    theta,residuals,rank,s = np.linalg.lstsq(X_train, y_train)\n",
    "    \n",
    "    theta = np.matrix(theta).T\n",
    "    X_test = np.matrix(X_test)\n",
    "    y_test = np.matrix(y_test).T\n",
    "    y_pred = X_test*theta\n",
    "    \n",
    "    return MSE(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First test sample 1 a) feature:\n",
      "[743, 9]\n",
      "\n",
      "First test sample 1 b) feature:\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "First test sample 1 c) feature:\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print('First test sample 1 a) feature:\\n{}'.format(feat1a(train[0])))\n",
    "print('\\nFirst test sample 1 b) feature:\\n{}'.format(feat1b(train[0])))\n",
    "print('\\nFirst test sample 1 c) feature:\\n{}'.format(feat1c(train[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse1a = experiment(a=True, b=False, c=False)\n",
    "mse1b = experiment(a=False, b=True, c=False)\n",
    "mse1c = experiment(a=False, b=False, c=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on test data for 1 a) model: 6169.549296366498\n",
      "MSE on test data for 1 b) model: 6396.833687711815\n",
      "MSE on test data for 1 c) model: 6000.948439855973\n"
     ]
    }
   ],
   "source": [
    "print('MSE on test data for 1 a) model: {}'.format(mse1a))\n",
    "print('MSE on test data for 1 b) model: {}'.format(mse1b))\n",
    "print('MSE on test data for 1 c) model: {}'.format(mse1c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "- TODO: write about abelation studies here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "mseall = experiment(a=True,b=True,c=True)\n",
    "mse1a_excluded = experiment(a=False,b=True,c=True)\n",
    "mse1b_excluded = experiment(a=True,b=False,c=True)\n",
    "mse1c_excluded = experiment(a=True,b=True,c=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on test data for model including all features: 5861.253905671382\n",
      "MSE on test data for model excluding 1 a) features: 5992.6635101007005\n",
      "MSE on test data for model excluding 1 b) features: 5870.11506165606\n",
      "MSE on test data for model excluding 1 c) features: 6157.754094366207\n"
     ]
    }
   ],
   "source": [
    "print('MSE on test data for model including all features: {}'.format(mseall))\n",
    "print('MSE on test data for model excluding 1 a) features: {}'.format(mse1a_excluded))\n",
    "print('MSE on test data for model excluding 1 b) features: {}'.format(mse1b_excluded))\n",
    "print('MSE on test data for model excluding 1 c) features: {}'.format(mse1c_excluded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "- TODO: write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BER(predictions, y):\n",
    "    # Implement following this logic or otherwise\n",
    "    TP = sum([(p and l) for (p,l) in zip(predictions, y)])\n",
    "    FP = sum([(p and (not l)) for (p,l) in zip(predictions, y)])\n",
    "    TN = sum([((not p) and (not l)) for (p,l) in zip(predictions, y)])\n",
    "    FN = sum([((not p) and l) for (p,l) in zip(predictions, y)])\n",
    "    return 1 - 0.5 * (TP / (TP + FN) + TN / (TN + FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mostPopularInd(dict_size):\n",
    "    i = 0\n",
    "    mostPopularInd = {}\n",
    "    idx = 0\n",
    "    while i < dict_size and idx < len(ingredients_list):\n",
    "        ingredient = ingredients_list[idx][1]\n",
    "        idx += 1\n",
    "        if ingredient == 'butter':\n",
    "            continue\n",
    "        mostPopularInd[ingredient] = i\n",
    "        i += 1\n",
    "    \n",
    "    return mostPopularInd\n",
    "\n",
    "def feat2(d, dict_size, mostPopularInd):\n",
    "    fIng = [0] * dict_size\n",
    "    for i in d['ingredients']:\n",
    "        if i == 'butter':\n",
    "            continue\n",
    "        if i in mostPopularInd:\n",
    "            fIng[mostPopularInd[i]] = 1\n",
    "    return fIng\n",
    "\n",
    "def hasButter(d):\n",
    "    if 'butter' in d['ingredients']:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(reg = 1, dict_size = 50, mode='valid'):\n",
    "    # Hint: run an experiment with a particular regularization strength, and a particular one-hot encoding size\n",
    "    # extract features...\n",
    "    # (etc.)\n",
    "    mostPopularInd = get_mostPopularInd(dict_size)\n",
    "    X_train = [feat2(d, dict_size, mostPopularInd) for d in train]\n",
    "    Y_train = [hasButter(d) for d in train]\n",
    "    mod = linear_model.LogisticRegression(C=reg, class_weight='balanced', solver = 'lbfgs')\n",
    "    mod.fit(X_train, Y_train)\n",
    "    Y_train_pred = mod.predict(X_train)\n",
    "    \n",
    "    if mode == 'valid':\n",
    "        X_valid = [feat2(d, dict_size, mostPopularInd) for d in valid]\n",
    "        Y_valid = [hasButter(d) for d in valid]\n",
    "        Y_pred = mod.predict(X_valid)\n",
    "        return BER(Y_train_pred, Y_train), BER(Y_pred, Y_valid)\n",
    "    \n",
    "    X_test = [feat2(d, dict_size, mostPopularInd) for d in test]\n",
    "    Y_test = [hasButter(d) for d in test]\n",
    "    Y_pred = mod.predict(X_test)\n",
    "    return BER(Y_train_pred, Y_train), BER(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Error Rate of the classifier on the test set: 0.28930328282968243\n"
     ]
    }
   ],
   "source": [
    "ber5_train, ber5_test = experiment(reg=1, dict_size=50, mode='test')\n",
    "print('Balanced Error Rate of the classifier on the test set: {}'.format(ber5_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "    C_best = None\n",
    "    dsize_best = None\n",
    "    ber_best = None\n",
    "    for C in [1e-3, 1e-2, 1e-1, 1e0, 10, 100, 1000]:\n",
    "        for dsize in [50, 100, 500]:\n",
    "            ber_train, ber_cur = experiment(reg=C, dict_size=dsize, mode='valid')\n",
    "            print('C: {0:5}, dict size: {1:5}, BER on train set: {2:10}, BER on valid set: {3:10}'\n",
    "                  .format(C, dsize, round(ber_train, 5), round(ber_cur, 5)))\n",
    "            if (ber_best == None) or (ber_cur < ber_best):\n",
    "                C_best = C\n",
    "                dsize_best = dsize\n",
    "                ber_best = ber_cur\n",
    "    \n",
    "    return C_best, dsize_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.001, dict size:    50, BER on train set:    0.30158, BER on valid set:    0.30632\n",
      "C: 0.001, dict size:   100, BER on train set:    0.28364, BER on valid set:    0.28685\n",
      "C: 0.001, dict size:   500, BER on train set:     0.2635, BER on valid set:    0.26701\n",
      "C:  0.01, dict size:    50, BER on train set:    0.29019, BER on valid set:    0.29033\n",
      "C:  0.01, dict size:   100, BER on train set:    0.26405, BER on valid set:    0.26473\n",
      "C:  0.01, dict size:   500, BER on train set:    0.22835, BER on valid set:    0.23016\n",
      "C:   0.1, dict size:    50, BER on train set:    0.28989, BER on valid set:    0.28972\n",
      "C:   0.1, dict size:   100, BER on train set:    0.26198, BER on valid set:      0.264\n",
      "C:   0.1, dict size:   500, BER on train set:    0.22355, BER on valid set:    0.22469\n",
      "C:   1.0, dict size:    50, BER on train set:    0.28988, BER on valid set:    0.28952\n",
      "C:   1.0, dict size:   100, BER on train set:     0.2622, BER on valid set:    0.26447\n",
      "C:   1.0, dict size:   500, BER on train set:    0.22311, BER on valid set:    0.22518\n",
      "C:    10, dict size:    50, BER on train set:    0.28992, BER on valid set:    0.28952\n",
      "C:    10, dict size:   100, BER on train set:    0.26224, BER on valid set:     0.2643\n",
      "C:    10, dict size:   500, BER on train set:     0.2231, BER on valid set:    0.22514\n",
      "C:   100, dict size:    50, BER on train set:    0.28992, BER on valid set:    0.28952\n",
      "C:   100, dict size:   100, BER on train set:    0.26221, BER on valid set:    0.26441\n",
      "C:   100, dict size:   500, BER on train set:    0.22314, BER on valid set:    0.22506\n",
      "C:  1000, dict size:    50, BER on train set:    0.28996, BER on valid set:    0.28954\n",
      "C:  1000, dict size:   100, BER on train set:    0.26221, BER on valid set:    0.26441\n",
      "C:  1000, dict size:   500, BER on train set:     0.2231, BER on valid set:    0.22512\n"
     ]
    }
   ],
   "source": [
    "C_opt, dsize_opt = pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best value of C:   0.1, best value of dict size:   500\n",
      "Balanced Error Rate of the classifier on the test set for selected model: 0.22504702608007565\n"
     ]
    }
   ],
   "source": [
    "ber6_train, ber6_test = experiment(reg=C_opt, dict_size=dsize_opt, mode='test')\n",
    "print('best value of C: {0:5}, best value of dict size: {1:5}'.format(C_opt, dsize_opt))\n",
    "print('Balanced Error Rate of the classifier on the test set for selected model: {0:10}'.format(ber6_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 (Recommender Systems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility data structures\n",
    "ingsPerItem = defaultdict(set)\n",
    "itemsPerIng = defaultdict(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    r = d['recipe_id']\n",
    "    for i in d['ingredients']:\n",
    "        ingsPerItem[r].add(i)\n",
    "        itemsPerIng[i].add(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_comparator(tuple1, tuple2):\n",
    "    if tuple1[0] > tuple2[0]:\n",
    "        return -1\n",
    "    elif tuple1[0] < tuple2[0]:\n",
    "        return 1\n",
    "    elif tuple1[1] < tuple2[1]:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def mostSimilar8(i, N):\n",
    "    similarities = []\n",
    "    ings = ingsPerItem[i]\n",
    "    for i2 in ingsPerItem:\n",
    "        if i2 == i: continue\n",
    "        sim = Jaccard(ings, ingsPerItem[i2])\n",
    "        similarities.append((sim,i2))\n",
    "    similarities.sort(key=functools.cmp_to_key(similarity_comparator))\n",
    "    return similarities[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing (similarity, recipe id) values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.4166666666666667, '68523854'),\n",
       " (0.38461538461538464, '12679596'),\n",
       " (0.36363636363636365, '56301588'),\n",
       " (0.36363636363636365, '79675099'),\n",
       " (0.35714285714285715, '87359281')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_id = '06432987'\n",
    "print('Printing (similarity, recipe id) values')\n",
    "mostSimilar8(recipe_id, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilar9(i, N):\n",
    "    similarities = []\n",
    "    items = itemsPerIng[i]\n",
    "    for i2 in itemsPerIng:\n",
    "        if i2 == i: continue\n",
    "        sim = Jaccard(items, itemsPerIng[i2])\n",
    "        similarities.append((sim,i2))\n",
    "    similarities.sort(key=functools.cmp_to_key(similarity_comparator))\n",
    "    return similarities[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing (similarity, ingredient id) values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.22315311514274808, 'salt'),\n",
       " (0.2056685424969639, 'flour'),\n",
       " (0.19100394157199166, 'eggs'),\n",
       " (0.17882420717656095, 'sugar'),\n",
       " (0.17040052045973944, 'milk')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredient_id = 'butter'\n",
    "print('Printing (similarity, ingredient id) values')\n",
    "mostSimilar9(ingredient_id, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 10\n",
    "#(open ended)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
